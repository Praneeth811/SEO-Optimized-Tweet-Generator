# -*- coding: utf-8 -*-
"""Copy of tweet_01_27/06.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxeFmfDTtJ00_nTIM46wrbWshlY_Xxfm
"""

!pip install python-docx scikit-learn --quiet

import io
import re
import pandas as pd
import docx
from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from google.colab import files

uploaded = files.upload()
filename = next(iter(uploaded))
ext = filename.split('.')[-1].lower()

def extract_text_from_file(file_bytes, ext):
    if ext == 'docx':
        doc = docx.Document(io.BytesIO(file_bytes))
        return ' '.join([para.text for para in doc.paragraphs if para.text.strip()])
    elif ext == 'csv':
        df = pd.read_csv(io.BytesIO(file_bytes))
        return ' '.join(df.astype(str).fillna('').apply(lambda x: ' '.join(x), axis=1))
    else:
        raise ValueError("Unsupported file format. Please upload .docx or .csv.")

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip()

def extract_keywords(text, top_n=5):
    tfidf = TfidfVectorizer(stop_words='english')
    vectors = tfidf.fit_transform([text])
    scores = zip(tfidf.get_feature_names_out(), vectors.toarray()[0])
    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)
    return [word.lower() for word, score in sorted_scores[:top_n]]

emoji_map = {
    "ai": "ğŸ¤–", "openai": "ğŸ§ ", "gpt": "ğŸ’¬", "healthcare": "ğŸ¥", "finance": "ğŸ’°",
    "education": "ğŸ“š", "robot": "ğŸ¤–", "data": "ğŸ“Š", "tech": "âš™ï¸", "future": "ğŸ”®"
}

def match_emoji(keywords):
    return ' '.join(emoji_map.get(kw.lower(), '') for kw in keywords if kw.lower() in emoji_map).strip()

def generate_all_tweets(text, keywords, max_chars=500):
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    emoji = match_emoji(keywords)
    org = "OpenAI" if "openai" in keywords else keywords[0].capitalize()
    topic = ", ".join(keywords[:2])
    hashtags = f"#AI #{org}"

    base_tweets = {
    "Professional": f"Here's what you need to know about {keywords[0]} and relevance of {keywords[0]} today. ",
    "Viral/Bold": f"{keywords[0].capitalize()} is catching everyone's attention â€” and for good reason. ğŸ”¥ ",
    "Curious/Teaser": f"Wondering why {topic} keeps coming up? This will clear things up. ğŸ‘‡ ",
    "News-style": f"Recent highlights around {keywords[0]} show just how impactful it's become. "
}



    tweets = {}

    for tone, base in base_tweets.items():
        body = ""
        for s in sentences:
            if len(base + body + hashtags + emoji) + len(s) + 3 <= max_chars:
                body += s + " "
            else:
                break
        tweet = f"{base}{body.strip()} {hashtags} {emoji}".strip()
        tweets[tone] = tweet[:max_chars]
    return tweets


def save_all_to_docx(tweets, filename="all_optimized_tweets.docx"):
    doc = Document()
    doc.add_heading('SEO-Optimized Tweet Variants', level=1)
    for tone, tweet in tweets.items():
        doc.add_heading(f"{tone} Style", level=2)
        doc.add_paragraph(tweet)
    doc.save(filename)
    files.download(filename)

file_content = uploaded[filename]
text = clean_text(extract_text_from_file(file_content, ext))
keywords = extract_keywords(text)
tweets = generate_all_tweets(text, keywords)

print("\nğŸ“¢ Generated Tweet Variants:\n")
for tone, tweet in tweets.items():
    print(f"{tone}:\n{tweet}\n")

save_all_to_docx(tweets)

